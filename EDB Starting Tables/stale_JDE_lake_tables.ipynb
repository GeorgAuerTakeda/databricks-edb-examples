{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ebb6cb-4a82-4cd5-9e25-b6a0e71565e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Python script to identify which jde_proddta tables where data seems to be \"stale\" (no updates today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e79418a-6a64-4ba7-8c6c-2ed8a064dafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, current_date, to_date\n",
    "\n",
    "# Function to find all tables with \"proddta\" in their names\n",
    "def find_proddta_tables(schema_name):\n",
    "    tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n",
    "    proddta_tables = tables_df.filter(col(\"tableName\").like(\"%proddta%\")) \\\n",
    "                              .select(\"tableName\") \\\n",
    "                              .collect()\n",
    "    return [f\"{schema_name}.{row['tableName']}\" for row in proddta_tables]\n",
    "\n",
    "# Use the function to get the list of tables\n",
    "tables = find_proddta_tables(\"gms_us_lake\")\n",
    "\n",
    "# Function to check if a table has data from today\n",
    "def check_table(table_name):\n",
    "    df = spark.table(table_name)\n",
    "    if df.filter(to_date(col(\"AUD_LD_DTS\")) == current_date()).count() > 0:\n",
    "        return (table_name, \"Updated Today\")\n",
    "    else:\n",
    "        return (table_name, \"Stale Data\")\n",
    "\n",
    "# Check each table and collect results\n",
    "results = [check_table(table) for table in tables]\n",
    "\n",
    "# Convert results to a DataFrame for display\n",
    "results_df = spark.createDataFrame(results, [\"table_name\", \"status\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390d12c9-a6ed-415b-88c7-d6e94acd4b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, current_date, to_date, regexp_extract\n",
    "\n",
    "# Function to find all tables with \"proddta\" in their names\n",
    "def find_proddta_tables(schema_name):\n",
    "    tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n",
    "    proddta_tables = tables_df.filter(col(\"tableName\").like(\"%erp_glbl%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%bckup%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%bkp%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%bck%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%dummy%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%test%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%phase%\")) \\\n",
    "                              .select(\"tableName\") \\\n",
    "                              .collect()\n",
    "    return [f\"{schema_name}.{row['tableName']}\" for row in proddta_tables]\n",
    "\n",
    "# Use the function to get the list of tables\n",
    "tables = find_proddta_tables(\"gms_us_hub\")\n",
    "\n",
    "# Function to check if a table has data from today and extract distinct values\n",
    "def check_table(table_name):\n",
    "    df = spark.table(table_name)\n",
    "    if \"AUD_LD_DTS\" in df.columns:\n",
    "        if df.filter(to_date(col(\"AUD_LD_DTS\")) == current_date()).count() > 0:\n",
    "            status = \"Updated Today\"\n",
    "        else:\n",
    "            status = \"Stale Data\"\n",
    "    else:\n",
    "        status = \"Column Not Found\"\n",
    "    \n",
    "    if \"AUD_FILE_NM\" in df.columns:\n",
    "        distinct_values = df.select(regexp_extract(col(\"AUD_FILE_NM\"), 'PRODDTA/([^/]+)/', 1).alias(\"extracted_value\")).distinct().collect()\n",
    "        extracted_values = [row[\"extracted_value\"] for row in distinct_values if row[\"extracted_value\"]]\n",
    "        extracted_value = extracted_values[0] if extracted_values else \"N/A\"\n",
    "    else:\n",
    "        extracted_value = \"N/A\"\n",
    "    \n",
    "    return (table_name, status, extracted_value)\n",
    "\n",
    "# Check each table and collect results\n",
    "hub_erp_results = [check_table(table) for table in tables]\n",
    "\n",
    "# Convert results to a DataFrame for display\n",
    "erp_results_df = spark.createDataFrame(hub_erp_results, [\"table_name\", \"status\", \"extracted_value\"])\n",
    "display(erp_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c53a6d4-94c1-4e0f-ad5b-11e409b2bde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f117d1-bfb4-48b8-8eac-6ffe56f2d2a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, current_date, to_date, regexp_extract\n",
    "\n",
    "# Function to find all tables with \"proddta\" in their names\n",
    "def find_proddta_tables(schema_name):\n",
    "    tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n",
    "    proddta_tables = tables_df.filter(col(\"tableName\").like(\"%proddta%\")) \\\n",
    "                              .select(\"tableName\") \\\n",
    "                              .collect()\n",
    "    return [f\"{schema_name}.{row['tableName']}\" for row in proddta_tables]\n",
    "\n",
    "# Use the function to get the list of tables\n",
    "tables = find_proddta_tables(\"gms_us_lake\")\n",
    "\n",
    "# Function to check if a table has data from today\n",
    "def check_table(table_name):\n",
    "    df = spark.table(table_name)\n",
    "    if df.filter(to_date(col(\"AUD_LD_DTS\")) == current_date()).count() > 0:\n",
    "        return (table_name, \"Updated Today\")\n",
    "    else:\n",
    "        return (table_name, \"Stale Data\")\n",
    "\n",
    "# Check each table and collect results\n",
    "results = [check_table(table) for table in tables]\n",
    "\n",
    "# Convert results to a DataFrame for display\n",
    "results_df = spark.createDataFrame(results, [\"table_name_lake\", \"status_lake\"])\n",
    "display(results_df)\n",
    "\n",
    "# Function to find all tables with \"proddta\" in their names\n",
    "def find_proddta_tables(schema_name):\n",
    "    tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n",
    "    proddta_tables = tables_df.filter(col(\"tableName\").like(\"%erp_glbl%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%bckup%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%bkp%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%bck%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%dummy%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%test%\")) \\\n",
    "                              .filter(~col(\"tableName\").like(\"%phase%\")) \\\n",
    "                              .select(\"tableName\") \\\n",
    "                              .collect()\n",
    "    return [f\"{schema_name}.{row['tableName']}\" for row in proddta_tables]\n",
    "\n",
    "# Use the function to get the list of tables\n",
    "tables = find_proddta_tables(\"gms_us_hub\")\n",
    "\n",
    "# Function to check if a table has data from today and extract distinct values\n",
    "def check_table(table_name):\n",
    "    df = spark.table(table_name)\n",
    "    if \"AUD_LD_DTS\" in df.columns:\n",
    "        if df.filter(to_date(col(\"AUD_LD_DTS\")) == current_date()).count() > 0:\n",
    "            status = \"Updated Today\"\n",
    "        else:\n",
    "            status = \"Stale Data\"\n",
    "    else:\n",
    "        status = \"Column Not Found\"\n",
    "    \n",
    "    if \"AUD_FILE_NM\" in df.columns:\n",
    "        distinct_values = df.select(regexp_extract(col(\"AUD_FILE_NM\"), 'PRODDTA/([^/]+)/', 1).alias(\"extracted_value\")).distinct().collect()\n",
    "        extracted_values = [row[\"extracted_value\"] for row in distinct_values if row[\"extracted_value\"]]\n",
    "        extracted_value = extracted_values[0] if extracted_values else \"N/A\"\n",
    "    else:\n",
    "        extracted_value = \"N/A\"\n",
    "    \n",
    "    return (table_name, status, extracted_value)\n",
    "\n",
    "# Check each table and collect results\n",
    "hub_erp_results = [check_table(table) for table in tables]\n",
    "\n",
    "# Convert results to a DataFrame for display\n",
    "erp_results_df = spark.createDataFrame(hub_erp_results, [\"table_name\", \"status_hub\", \"extracted_value\"])\n",
    "display(erp_results_df)\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "joined_df = erp_results_df.join(\n",
    "    results_df,\n",
    "    expr(\"table_name like concat('%', extracted_value, '%')\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    erp_results_df[\"table_name\"],\n",
    "    erp_results_df[\"status_hub\"],\n",
    "    results_df[\"table_name_lake\"],\n",
    "    results_df[\"status_lake\"]\n",
    ")\n",
    "\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "939fecbc-082f-4761-b6ba-ec405fcd12a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# which JDE tables in hub are up to date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3744641c-2431-4868-b18f-96a7aceb0a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import lower, col, expr\n",
    "\n",
    "joined_df = erp_results_df.join(\n",
    "    results_df,\n",
    "    expr(\"lower(table_name_lake) like concat('%', lower(extracted_value), '%')\"),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "828026fd-a20e-483e-b1fd-2297222d176a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select distinct AUD_FILE_NM, REGEXP_EXTRACT(AUD_FILE_NM, 'PRODDTA/([^/]+)/', 1) AS extracted_string from gms_us_hub.ref_plantcode_masterdata_erp_glbl\n",
    "where AUD_FILE_NM like '%PRODDTA%'\n",
    "-- limit 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33780a86-4c01-4c8c-a6b0-9a29985593c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select distinct REGEXP_EXTRACT(AUD_FILE_NM, 'PRODDTA/([^/]+)/', 1) AS extracted_string from gms_us_hub.txn_stockmovement_erp_glbl\n",
    "where AUD_FILE_NM like '%PRODDTA%'\n",
    "-- limit 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6905d884-e7a7-4b30-978a-8c60d0bc83ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# sql solution - not enough rights to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2bb9cc-3431-4855-a436-e682a37f2eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT table_name, last_altered\n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = 'gms_us_lake'\n",
    "  AND table_name LIKE '%proddta%'\n",
    "  AND DATE(last_altered) = '2025-06-13';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4587ec15-65a5-4da8-8a58-be28bf5df974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT table_name, last_altered\n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = 'gms_us_lake'\n",
    "  AND table_name LIKE '%jdf_proddta%'\n",
    "  AND DATE(last_altered) != '2025-06-13';"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stale_JDE_lake_tables",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
